{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text\n",
    "with open('../../deep-learning-v2-pytorch/recurrent-neural-networks/char-rnn/data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 100\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 68, 70, 25,  5, 55, 81, 60, 39, 82, 82, 82, 53, 70, 25, 25, 26,\n",
       "       60, 79, 70, 23, 27,  8, 27, 55,  7, 60, 70, 81, 55, 60, 70,  8,  8,\n",
       "       60, 70,  8, 27, 64, 55,  6, 60, 55,  9, 55, 81, 26, 60, 40, 76, 68,\n",
       "       70, 25, 25, 26, 60, 79, 70, 23, 27,  8, 26, 60, 27,  7, 60, 40, 76,\n",
       "       68, 70, 25, 25, 26, 60, 27, 76, 60, 27,  5,  7, 60, 42, 24, 76, 82,\n",
       "       24, 70, 26,  2, 82, 82, 71,  9, 55, 81, 26,  5, 68, 27, 76])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "def one_hot_encode(arr, labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape),labels), dtype=np.float32 )\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = np.array([[1,2,5]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # get the number of full batches\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    # keep only enough chars to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # reshape into {batch_size} rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    # loop through the batches using a seq length of 3\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[77 68 70 25  5 55 81 60 39 82]\n",
      " [ 7 42 76 60  5 68 70  5 60 70]\n",
      " [55 76 11 60 42 81 60 70 60 79]\n",
      " [ 7 60  5 68 55 60 57 68 27 55]\n",
      " [60  7 70 24 60 68 55 81 60  5]\n",
      " [57 40  7  7 27 42 76 60 70 76]\n",
      " [60 78 76 76 70 60 68 70 11 60]\n",
      " [63 32  8 42 76  7 64 26  2 60]]\n",
      "\n",
      "y\n",
      " [[68 70 25  5 55 81 60 39 82 82]\n",
      " [42 76 60  5 68 70  5 60 70  5]\n",
      " [76 11 60 42 81 60 70 60 79 42]\n",
      " [60  5 68 55 60 57 68 27 55 79]\n",
      " [ 7 70 24 60 68 55 81 60  5 55]\n",
      " [40  7  7 27 42 76 60 70 76 11]\n",
      " [78 76 76 70 60 68 70 11 60  7]\n",
      " [32  8 42 76  7 64 26  2 60 28]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.n_layers = n_layers\n",
    "            self.n_hidden = n_hidden\n",
    "            self.lr = lr\n",
    "            \n",
    "            #create char dictionaries\n",
    "            self.chars = tokens\n",
    "            self.int2char = dict(enumerate(self.chars))\n",
    "            self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "            \n",
    "            # model layers\n",
    "            self.lstm = nn.LSTM(len(self.chars),n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "            \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''initializes hidden state'''\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "\n",
    "        return hidden\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    net.to(device)\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 2.1378... Val Loss: 2.1357\n",
      "Epoch: 1/20... Step: 20... Loss: 2.0948... Val Loss: 2.1000\n",
      "Epoch: 1/20... Step: 30... Loss: 2.0862... Val Loss: 2.0705\n",
      "Epoch: 1/20... Step: 40... Loss: 2.0376... Val Loss: 2.0529\n",
      "Epoch: 1/20... Step: 50... Loss: 2.0578... Val Loss: 2.0363\n",
      "Epoch: 1/20... Step: 60... Loss: 1.9831... Val Loss: 2.0155\n",
      "Epoch: 1/20... Step: 70... Loss: 1.9877... Val Loss: 2.0023\n",
      "Epoch: 1/20... Step: 80... Loss: 1.9637... Val Loss: 1.9889\n",
      "Epoch: 1/20... Step: 90... Loss: 1.9764... Val Loss: 1.9679\n",
      "Epoch: 1/20... Step: 100... Loss: 1.9306... Val Loss: 1.9502\n",
      "Epoch: 1/20... Step: 110... Loss: 1.9089... Val Loss: 1.9346\n",
      "Epoch: 1/20... Step: 120... Loss: 1.8645... Val Loss: 1.9242\n",
      "Epoch: 1/20... Step: 130... Loss: 1.9011... Val Loss: 1.9029\n",
      "Epoch: 2/20... Step: 140... Loss: 1.9013... Val Loss: 1.8843\n",
      "Epoch: 2/20... Step: 150... Loss: 1.8655... Val Loss: 1.8697\n",
      "Epoch: 2/20... Step: 160... Loss: 1.8662... Val Loss: 1.8559\n",
      "Epoch: 2/20... Step: 170... Loss: 1.8385... Val Loss: 1.8391\n",
      "Epoch: 2/20... Step: 180... Loss: 1.7963... Val Loss: 1.8286\n",
      "Epoch: 2/20... Step: 190... Loss: 1.7456... Val Loss: 1.8105\n",
      "Epoch: 2/20... Step: 200... Loss: 1.7515... Val Loss: 1.7970\n",
      "Epoch: 2/20... Step: 210... Loss: 1.7517... Val Loss: 1.7845\n",
      "Epoch: 2/20... Step: 220... Loss: 1.7345... Val Loss: 1.7748\n",
      "Epoch: 2/20... Step: 230... Loss: 1.7458... Val Loss: 1.7574\n",
      "Epoch: 2/20... Step: 240... Loss: 1.7374... Val Loss: 1.7499\n",
      "Epoch: 2/20... Step: 250... Loss: 1.6927... Val Loss: 1.7350\n",
      "Epoch: 2/20... Step: 260... Loss: 1.6626... Val Loss: 1.7248\n",
      "Epoch: 2/20... Step: 270... Loss: 1.7037... Val Loss: 1.7102\n",
      "Epoch: 3/20... Step: 280... Loss: 1.6784... Val Loss: 1.6960\n",
      "Epoch: 3/20... Step: 290... Loss: 1.6845... Val Loss: 1.6864\n",
      "Epoch: 3/20... Step: 300... Loss: 1.6417... Val Loss: 1.6719\n",
      "Epoch: 3/20... Step: 310... Loss: 1.6397... Val Loss: 1.6650\n",
      "Epoch: 3/20... Step: 320... Loss: 1.5974... Val Loss: 1.6582\n",
      "Epoch: 3/20... Step: 330... Loss: 1.5939... Val Loss: 1.6477\n",
      "Epoch: 3/20... Step: 340... Loss: 1.6460... Val Loss: 1.6379\n",
      "Epoch: 3/20... Step: 350... Loss: 1.5930... Val Loss: 1.6267\n",
      "Epoch: 3/20... Step: 360... Loss: 1.5527... Val Loss: 1.6241\n",
      "Epoch: 3/20... Step: 370... Loss: 1.5761... Val Loss: 1.6132\n",
      "Epoch: 3/20... Step: 380... Loss: 1.5786... Val Loss: 1.6069\n",
      "Epoch: 3/20... Step: 390... Loss: 1.5593... Val Loss: 1.6028\n",
      "Epoch: 3/20... Step: 400... Loss: 1.5345... Val Loss: 1.5880\n",
      "Epoch: 3/20... Step: 410... Loss: 1.5466... Val Loss: 1.5832\n",
      "Epoch: 4/20... Step: 420... Loss: 1.5400... Val Loss: 1.5713\n",
      "Epoch: 4/20... Step: 430... Loss: 1.5389... Val Loss: 1.5696\n",
      "Epoch: 4/20... Step: 440... Loss: 1.5290... Val Loss: 1.5612\n",
      "Epoch: 4/20... Step: 450... Loss: 1.4872... Val Loss: 1.5534\n",
      "Epoch: 4/20... Step: 460... Loss: 1.4603... Val Loss: 1.5499\n",
      "Epoch: 4/20... Step: 470... Loss: 1.5222... Val Loss: 1.5429\n",
      "Epoch: 4/20... Step: 480... Loss: 1.5042... Val Loss: 1.5389\n",
      "Epoch: 4/20... Step: 490... Loss: 1.4966... Val Loss: 1.5271\n",
      "Epoch: 4/20... Step: 500... Loss: 1.5119... Val Loss: 1.5284\n",
      "Epoch: 4/20... Step: 510... Loss: 1.4775... Val Loss: 1.5184\n",
      "Epoch: 4/20... Step: 520... Loss: 1.4951... Val Loss: 1.5138\n",
      "Epoch: 4/20... Step: 530... Loss: 1.4698... Val Loss: 1.5086\n",
      "Epoch: 4/20... Step: 540... Loss: 1.4282... Val Loss: 1.4993\n",
      "Epoch: 4/20... Step: 550... Loss: 1.5000... Val Loss: 1.4972\n",
      "Epoch: 5/20... Step: 560... Loss: 1.4505... Val Loss: 1.4921\n",
      "Epoch: 5/20... Step: 570... Loss: 1.4396... Val Loss: 1.4875\n",
      "Epoch: 5/20... Step: 580... Loss: 1.4379... Val Loss: 1.4830\n",
      "Epoch: 5/20... Step: 590... Loss: 1.4300... Val Loss: 1.4768\n",
      "Epoch: 5/20... Step: 600... Loss: 1.4233... Val Loss: 1.4794\n",
      "Epoch: 5/20... Step: 610... Loss: 1.4001... Val Loss: 1.4710\n",
      "Epoch: 5/20... Step: 620... Loss: 1.4186... Val Loss: 1.4721\n",
      "Epoch: 5/20... Step: 630... Loss: 1.4286... Val Loss: 1.4644\n",
      "Epoch: 5/20... Step: 640... Loss: 1.3998... Val Loss: 1.4622\n",
      "Epoch: 5/20... Step: 650... Loss: 1.4111... Val Loss: 1.4523\n",
      "Epoch: 5/20... Step: 660... Loss: 1.3897... Val Loss: 1.4522\n",
      "Epoch: 5/20... Step: 670... Loss: 1.4069... Val Loss: 1.4502\n",
      "Epoch: 5/20... Step: 680... Loss: 1.4039... Val Loss: 1.4417\n",
      "Epoch: 5/20... Step: 690... Loss: 1.3921... Val Loss: 1.4433\n",
      "Epoch: 6/20... Step: 700... Loss: 1.3993... Val Loss: 1.4349\n",
      "Epoch: 6/20... Step: 710... Loss: 1.3809... Val Loss: 1.4325\n",
      "Epoch: 6/20... Step: 720... Loss: 1.3761... Val Loss: 1.4309\n",
      "Epoch: 6/20... Step: 730... Loss: 1.3800... Val Loss: 1.4279\n",
      "Epoch: 6/20... Step: 740... Loss: 1.3524... Val Loss: 1.4307\n",
      "Epoch: 6/20... Step: 750... Loss: 1.3483... Val Loss: 1.4215\n",
      "Epoch: 6/20... Step: 760... Loss: 1.3845... Val Loss: 1.4258\n",
      "Epoch: 6/20... Step: 770... Loss: 1.3612... Val Loss: 1.4180\n",
      "Epoch: 6/20... Step: 780... Loss: 1.3459... Val Loss: 1.4157\n",
      "Epoch: 6/20... Step: 790... Loss: 1.3415... Val Loss: 1.4116\n",
      "Epoch: 6/20... Step: 800... Loss: 1.3705... Val Loss: 1.4101\n",
      "Epoch: 6/20... Step: 810... Loss: 1.3388... Val Loss: 1.4108\n",
      "Epoch: 6/20... Step: 820... Loss: 1.3063... Val Loss: 1.4011\n",
      "Epoch: 6/20... Step: 830... Loss: 1.3559... Val Loss: 1.4012\n",
      "Epoch: 7/20... Step: 840... Loss: 1.3265... Val Loss: 1.3972\n",
      "Epoch: 7/20... Step: 850... Loss: 1.3333... Val Loss: 1.3952\n",
      "Epoch: 7/20... Step: 860... Loss: 1.3095... Val Loss: 1.3903\n",
      "Epoch: 7/20... Step: 870... Loss: 1.3240... Val Loss: 1.3908\n",
      "Epoch: 7/20... Step: 880... Loss: 1.3290... Val Loss: 1.3923\n",
      "Epoch: 7/20... Step: 890... Loss: 1.3300... Val Loss: 1.3836\n",
      "Epoch: 7/20... Step: 900... Loss: 1.3223... Val Loss: 1.3855\n",
      "Epoch: 7/20... Step: 910... Loss: 1.2964... Val Loss: 1.3821\n",
      "Epoch: 7/20... Step: 920... Loss: 1.3082... Val Loss: 1.3814\n",
      "Epoch: 7/20... Step: 930... Loss: 1.3017... Val Loss: 1.3785\n",
      "Epoch: 7/20... Step: 940... Loss: 1.3006... Val Loss: 1.3762\n",
      "Epoch: 7/20... Step: 950... Loss: 1.3130... Val Loss: 1.3722\n",
      "Epoch: 7/20... Step: 960... Loss: 1.3154... Val Loss: 1.3648\n",
      "Epoch: 7/20... Step: 970... Loss: 1.3178... Val Loss: 1.3712\n",
      "Epoch: 8/20... Step: 980... Loss: 1.2942... Val Loss: 1.3644\n",
      "Epoch: 8/20... Step: 990... Loss: 1.2960... Val Loss: 1.3629\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.2985... Val Loss: 1.3627\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.3239... Val Loss: 1.3626\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.3029... Val Loss: 1.3645\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.2884... Val Loss: 1.3533\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.2906... Val Loss: 1.3600\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.2825... Val Loss: 1.3572\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.2744... Val Loss: 1.3557\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.2866... Val Loss: 1.3531\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.2787... Val Loss: 1.3496\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.2760... Val Loss: 1.3502\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.2623... Val Loss: 1.3410\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.2672... Val Loss: 1.3449\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.2789... Val Loss: 1.3397\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.2697... Val Loss: 1.3356\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.2834... Val Loss: 1.3387\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.2880... Val Loss: 1.3393\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.2569... Val Loss: 1.3416\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.2619... Val Loss: 1.3355\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.2519... Val Loss: 1.3385\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.2896... Val Loss: 1.3345\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.2366... Val Loss: 1.3326\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.2433... Val Loss: 1.3290\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.2520... Val Loss: 1.3289\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.2331... Val Loss: 1.3265\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.2346... Val Loss: 1.3203\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.2439... Val Loss: 1.3197\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.2494... Val Loss: 1.3213\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.2457... Val Loss: 1.3144\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.2522... Val Loss: 1.3158\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.2444... Val Loss: 1.3150\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.2305... Val Loss: 1.3156\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.2393... Val Loss: 1.3107\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.2107... Val Loss: 1.3072\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.2066... Val Loss: 1.3074\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.2044... Val Loss: 1.3142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.2161... Val Loss: 1.3071\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.2106... Val Loss: 1.3045\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.1954... Val Loss: 1.3045\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.2395... Val Loss: 1.3005\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.2575... Val Loss: 1.2956\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.2445... Val Loss: 1.2968\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.2469... Val Loss: 1.2949\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.2536... Val Loss: 1.2914\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.2207... Val Loss: 1.2987\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.2292... Val Loss: 1.2982\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.1646... Val Loss: 1.2968\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.2024... Val Loss: 1.2952\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.1803... Val Loss: 1.2972\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.2149... Val Loss: 1.2960\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.1952... Val Loss: 1.2925\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.1908... Val Loss: 1.2972\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.1678... Val Loss: 1.2929\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.2023... Val Loss: 1.2957\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.2705... Val Loss: 1.2856\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.2123... Val Loss: 1.2844\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.2062... Val Loss: 1.2858\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.2170... Val Loss: 1.2853\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.1786... Val Loss: 1.2905\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.1752... Val Loss: 1.2893\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.1558... Val Loss: 1.2874\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.1826... Val Loss: 1.2836\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.1751... Val Loss: 1.2829\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.1702... Val Loss: 1.2831\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.1946... Val Loss: 1.2810\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.1686... Val Loss: 1.2818\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.1569... Val Loss: 1.2773\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.2072... Val Loss: 1.2835\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.1702... Val Loss: 1.2776\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.1915... Val Loss: 1.2784\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.1680... Val Loss: 1.2779\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.1594... Val Loss: 1.2761\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.1441... Val Loss: 1.2748\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.1542... Val Loss: 1.2796\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.1812... Val Loss: 1.2748\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.1606... Val Loss: 1.2719\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.1246... Val Loss: 1.2792\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.1536... Val Loss: 1.2787\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.1822... Val Loss: 1.2715\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.1632... Val Loss: 1.2774\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.1417... Val Loss: 1.2772\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.1654... Val Loss: 1.2706\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.1714... Val Loss: 1.2682\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.1445... Val Loss: 1.2661\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.1705... Val Loss: 1.2680\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.1193... Val Loss: 1.2698\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.1125... Val Loss: 1.2743\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.1621... Val Loss: 1.2793\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.1742... Val Loss: 1.2704\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.1497... Val Loss: 1.2650\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.1727... Val Loss: 1.2716\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.1582... Val Loss: 1.2659\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.1399... Val Loss: 1.2672\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.1596... Val Loss: 1.2699\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.1140... Val Loss: 1.2701\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.1710... Val Loss: 1.2632\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.1437... Val Loss: 1.2653\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.1406... Val Loss: 1.2592\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.1360... Val Loss: 1.2607\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.1237... Val Loss: 1.2596\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.1301... Val Loss: 1.2645\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.1277... Val Loss: 1.2630\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.1347... Val Loss: 1.2686\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.1538... Val Loss: 1.2570\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.1243... Val Loss: 1.2606\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.1444... Val Loss: 1.2628\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.1255... Val Loss: 1.2590\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.1409... Val Loss: 1.2621\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.1519... Val Loss: 1.2602\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.1468... Val Loss: 1.2593\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.1434... Val Loss: 1.2584\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.1326... Val Loss: 1.2541\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.1233... Val Loss: 1.2585\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.1287... Val Loss: 1.2615\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.1093... Val Loss: 1.2601\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.1233... Val Loss: 1.2576\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.1411... Val Loss: 1.2619\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.1241... Val Loss: 1.2535\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.1194... Val Loss: 1.2589\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.1153... Val Loss: 1.2588\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.1336... Val Loss: 1.2606\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.1180... Val Loss: 1.2553\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.0919... Val Loss: 1.2570\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.1243... Val Loss: 1.2575\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.1137... Val Loss: 1.2543\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.1199... Val Loss: 1.2522\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.0959... Val Loss: 1.2569\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.1167... Val Loss: 1.2510\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.1275... Val Loss: 1.2562\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.1235... Val Loss: 1.2536\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.1320... Val Loss: 1.2519\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.0895... Val Loss: 1.2514\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.0996... Val Loss: 1.2557\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.1043... Val Loss: 1.2513\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.0952... Val Loss: 1.2467\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.1201... Val Loss: 1.2516\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.1103... Val Loss: 1.2497\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.1135... Val Loss: 1.2534\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.0991... Val Loss: 1.2498\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.1055... Val Loss: 1.2502\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.1010... Val Loss: 1.2563\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.1246... Val Loss: 1.2513\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.1185... Val Loss: 1.2500\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.0958... Val Loss: 1.2503\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.1016... Val Loss: 1.2475\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.0934... Val Loss: 1.2501\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.0876... Val Loss: 1.2554\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.1012... Val Loss: 1.2565\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.1027... Val Loss: 1.2490\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.0997... Val Loss: 1.2667\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.0902... Val Loss: 1.2545\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.0895... Val Loss: 1.2511\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.0962... Val Loss: 1.2445\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.1133... Val Loss: 1.2499\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.1177... Val Loss: 1.2605\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.1159... Val Loss: 1.2466\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.0910... Val Loss: 1.2501\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.0938... Val Loss: 1.2464\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.0831... Val Loss: 1.2516\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.1133... Val Loss: 1.2502\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.0858... Val Loss: 1.2508\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.0740... Val Loss: 1.2598\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.0968... Val Loss: 1.2491\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.0778... Val Loss: 1.2599\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.0890... Val Loss: 1.2499\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.0915... Val Loss: 1.2550\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.0916... Val Loss: 1.2441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.1016... Val Loss: 1.2417\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.1067... Val Loss: 1.2495\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.0970... Val Loss: 1.2443\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.0812... Val Loss: 1.2498\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.0917... Val Loss: 1.2462\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.0669... Val Loss: 1.2487\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.0664... Val Loss: 1.2461\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.0637... Val Loss: 1.2464\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.0658... Val Loss: 1.2521\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.0790... Val Loss: 1.2525\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.0606... Val Loss: 1.2535\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.1015... Val Loss: 1.2489\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.1262... Val Loss: 1.2480\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden':net.n_hidden,\n",
    "             'n_layers':net.n_layers,\n",
    "             'state_dict':net.state_dict(),\n",
    "             'tokens':net.chars}\n",
    "with open('rnn_checkpoint', 'wb')as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        inputs = inputs.to(device)\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priming and generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna was\n",
      "to blame, the sorrow, and she would have thought of her. She\n",
      "had been so sorry for the strange sound in his heart, but he was not to\n",
      "see him, but he had to do with the soft of the satisfaction.\n",
      "\n",
      "\"Why, how is it, I am not so much to have the means to me in how\n",
      "sid I do it. I doesn't begin to be so much to me. But I wanted to say\n",
      "that you are no more and me for the most feeling.\"\n",
      "\n",
      "\"I don't understand it,\" she said to the letter, \"and the old prince is\n",
      "so sorry for her,\" and with a smile without and struck her and\n",
      "had no difference of some desire to spare a difficult expression\n",
      "of her face at the thick other and her feeling.\n",
      "\n",
      "The significance of the stream with his father had been all that he could\n",
      "never be done to be as a bit and his father, had, with the most\n",
      "part in his soul where they were satisfied with them, but she\n",
      "had been standing.\n",
      "\n",
      "Alexey Alexandrovitch stopped, she came to the soft of the party as to\n",
      "his face that she was always seen in the same son.\n",
      "\n",
      "\"What is it you advent me \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna was', top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs\n",
    "with open('rnn_checkpoint', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And she said to levin ...\n",
      "and was entering that time he heard her the charce. He was\n",
      "now and there was not the members of how to be living in the\n",
      "finances of her figure and the manner. The princess and\n",
      "the princess saw that she was always to get her thoughts. He\n",
      "was so good to stopped to her with a sort of study. He was\n",
      "awful, the sick man sat down at home, and was silent. Because\n",
      "he showed his head.\n",
      "\n",
      "\"Who has stoubed?\"\n",
      "\n",
      "\"No, it's all to go away, and I can't understand that it's not a\n",
      "possibility of concealing the crop, to be passing me if they're so till\n",
      "then to see you,\" said Vronsky, and her lows, were at the\n",
      "steps of the political expression, and so high todouhed her.\n",
      "\n",
      "Alexey Alexandrovitch had seen it any once made from the same\n",
      "peasants, who, she cried not to say and then at the same.\n",
      "\n",
      "\"It's a little garden.\"\n",
      "\n",
      "Anna had to get in sincere. But that it was all the contempt of\n",
      "pains. All that to start here and would not be asked to\n",
      "tell them.\n",
      "\n",
      "\"If it's the tries to thinks of my death,\" she said, so as not\n",
      "from her bothom and touching him for her ton.\n",
      "\n",
      "\"Ah, I shall be as too, in the side of the character which I have been to go\n",
      "away to me to begin it to be painted in the way has started over that\n",
      "shameful for you, but I shall come to see you,\" said Alexey\n",
      "Alexandrovitch. \"Italing they would be done for that? They are at the\n",
      "most feeling in the same doubt.\n",
      "If it were, and that's not silence. Well, then and see a more\n",
      "coldle, it was a serve of the father of all!\" said the closest of the\n",
      "bedroom, but at his head--to see.... But that is some\n",
      "country had a state of pressed on a long bottem, and that her\n",
      "feet, and he went batherone to temper a career of them in servants, and\n",
      "through her eyes were cold to the little path and to him to bring them\n",
      "away a concentrations of an answer with him there was a little, but he saw it with\n",
      "a few words there was nothing but the feeling of happiness.\n",
      "\n",
      "\"Well, then I have to do all together and night,\" he said, with tiny as\n",
      "she saw that her shart had been storted\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k=5, prime=\"And she said to levin ..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
